"start","content"
"0.0"," Welcome to this session on how to build a machine learning classifier in Python with"
"9.6"," scikit-learn."
"10.6"," In this session we'll take a look at what machine learning is and how you can use scikit-learn,"
"14.040000000000001"," which is one of the most popular libraries in Python for machine learning models to use"
"19.080000000000002"," scikit-learn package to build models that could classify the data that we sent to it."
"24.64"," Well, begin by taking a look at what is machine learning."
"28.2"," So machine learning is a fairly loaded term."
"30.68"," Many people use machine learning to mean a lot of different things."
"34.32"," So in this session, what we'll be doing is we'll first try and understand what machine"
"38.2"," learning is because that is the basis of the concepts that will be building things on."
"42.84"," So machine learning is the branch of computer science that deals with using data and mathematics"
"48.68"," to help computers learn rather than being explicitly programmed."
"52.84"," This is like the quintessential machine learning definition."
"55.64"," So to help understand it better, let's first understand what the traditional programming"
"59.480000000000004"," model is."
"60.480000000000004"," So machine learning has health developers, build tools and systems that could not have"
"65.6"," built by using traditional programming models."
"68.12"," So the reason why traditional programming models don't work is because we would explicitly"
"72.76"," have to come up with rules and put them in our system."
"75.88"," So to give you an example, if I were to ask you to create a machine learning, not machine"
"81.0"," learning, but create a system that would using traditional programming models recognize"
"86.08"," what a particular image, what is the digit that is contained inside a particular image"
"92.0"," that's a container 0, 1, 2, 3, 4, 5, 6, 7, 8, 9."
"97.32"," Now using that, you could probably define some constraint that the image needs to be"
"102.88"," 8 by 8 pixels."
"103.88"," It could only contain black and white."
"105.56"," But even with all those constraints, the variation is so much that it becomes really,"
"110.84"," really difficult for us to use traditional programming models to hard code some of the"
"115.2"," rules that will take in an image and will spit out the correct answer."
"119.32000000000001"," So to give you an example, if I were to give you a quick understanding of how this works,"
"124.80000000000001"," you would have to take the entire image, then take the data on each pixel and use the"
"129.92000000000002"," data somehow to understand what exactly is the number inside this image."
"135.04"," That is really difficult to do using traditional programming models."
"138.0"," While traditional programming models are good for several things, including building applications"
"142.8"," that could respond to particular events, particular data streams and a lot of other things,"
"148.04"," they are not good at changing with the requirements at the page machine learning allows them to."
"153.08"," So for instance, if I were to use machine learning, then our computers can actually generate"
"157.84"," some statistical models that will specify how confident it is that it is one of the following"
"164.52"," numbers from 0 to 9, which is a single digit number, which is a problem that we are trying"
"168.56"," to solve."
"169.56"," And it will solve it quite easily."
"171.36"," So that's why we use machine learning."
"173.16000000000003"," It allows developers to build programs that traditional programming models could be."
"176.88"," In traditional programming models, we would take in their data, run it through the entire"
"180.4"," program in which we will use data transformation and if else statements and many other programming"
"185.92000000000002"," constructs and using those will figure out what the use of ones and give them the output."
"191.48"," So we will include in machine learning models, what we do is we take an algorithm, machine"
"195.32"," learning algorithm, we take some data and we create a model, which then we'll take in the"
"199.95999999999998"," data and fit out the output."
"201.44"," So machine learning on its own the rules instead of all 7 to explicitly type out what the rules"
"206.23999999999998"," should be."
"207.23999999999998"," So we'll take a look at types of machine learning."
"210.88"," So there are three kinds of machine learning algorithms available or three broad categories"
"216.0"," in which machine learning algorithms can be divided."
"219.08"," These categories are divided based on the criteria of what kind of data we give them"
"224.08"," and how they learn and what kind of problems this all."
"227.52"," So the first one is supervised learning."
"230.48000000000002"," So if you take a look at supervised learning, it basically means that we take supervised"
"236.68"," learning is used to help machines learn how to map input to output based on the input"
"243.04000000000002"," output pairs that we have given to them as examples."
"246.2"," So to give you an idea of what supervised learning is in supervised learning, we give machine"
"251.67999999999998"," learning algorithms some data."
"254.04"," That data already has the x variables and the y variables."
"259.28"," Now x variables are the things that the machine learning model expects to be fed into it"
"264.28"," and the y variable is the thing that we're trying to predict."
"267.32"," So to give you an example, if you're trying to build a model that could predict the temperature"
"272.12"," of the upcoming seven days, let's say upcoming week, then the x variable would be the conditions"
"277.72"," that you're feeding into the data and these could be several different things."
"281.08"," That really depends on how you're training the model."
"283.6"," So for an example, if you're trying to train a model, try and understand what are the"
"288.32"," different kinds of things that can affect temperature."
"290.8"," So for instance, the altitude of the place that you're at, what was the temperature previous"
"295.32"," days, how were the weather conditions, what's the atmospheric pressure and all these kinds"
"301.32"," of things."
"302.08"," So that's how all combined into a row and that row is all the x variables and when we make"
"308.2"," a prediction on it, it will give us a temperature."
"310.56"," It could be in Celsius of Fahrenheit, depending on how you're training the model."
"313.94"," If you've trained the model with the Celsius data, it can't predict Fahrenheit."
"318.12"," You would have to convert it yourself."
"319.88"," And if it can, if you have trained it to Fahrenheit, it can't predict Celsius."
"324.32"," You would have to convert that to yourself."
"326.15999999999997"," So basically in supervised learning, what there, is we give machines some data in which"
"331.84"," the input and output are already present. The machine learning model then has to take a look at"
"337.84"," the data and understand how the input relates to the output come up with the rules on its own,"
"342.4"," so that when we feed in new data, it's able to predict the output or at least predict it"
"347.84"," towards now. So a good example of this would be if we show machine learning, for instance,"
"353.12"," when we're learning how to read alphabets, the first thing that they do is they show us a lot"
"357.6"," of alphabets how to write it. And so after this, we have unsupervised learning and unsupervised"
"365.44"," learning is quite easy to understand as well. Unsupervised learning is a machine learning technique"
"371.20000000000005"," which allows a model to work on its own to discover patterns and information that was previously"
"376.64000000000004"," undetected. To give you a bit of context here, my unsupervised learning, let's say that you have"
"382.88"," been given a lot of customer data, how customers have interacted with your website and how"
"388.08"," let's say you have an e-commerce website and how many customers have bought what kind of things"
"393.28"," and what are the kind of things that they've bought together. So some customers may have bought"
"398.64"," shirts and pants together, some customers may have bought gaming consoles and games together,"
"404.0"," these kinds of things. So what were these kinds of things that they've bought together and then"
"408.72"," after learning that you could come up with a strategy on creating offers. So if you are building"
"416.0"," that kind of a model that can just take a look at the data and then figure out some sort of pattern"
"420.24"," that's inside it. The data that you're giving doesn't contain the data, doesn't contain the"
"425.92"," pattern that you want it to find out. That's the main difference between supervised and unsupervised"
"430.16"," learning. In supervised learning, we give it all the data and all the outputs that we need from the"
"435.28"," data and it essentially works out of pattern to map the input from the output. In unsupervised"
"441.2"," learning, we just give it a lot of data and just tell it, okay, figure out some pattern that could"
"445.11999999999995"," help me out in building my business strategy or creating or understanding how the data works and"
"452.4"," how the trends are going and that way you can use that. So now what we have to do is after unsupervised"
"463.6"," learning, let's take a look at an example of unsupervised learning algorithm. So for instance,"
"468.16"," just like I told you, you can build a model that can help you generate offers. You can also build"
"474.16"," models that can help you group customers. So for instance, given a customer and his buying habit,"
"480.0"," you can group them into premium customers, you can group them into customers that you could"
"484.16"," follow up on to share exciting offers with or share a reward point for something like that."
"491.6"," So that also helps you out in case you're using machine learning. Then comes reinforcement learning."
"497.92"," Now this is quite an important kind of a thing. In reinforcement learning, what we're doing is"
"503.44"," we're teaching our modules how to make decisions. Now these techniques allow the use in agent"
"511.68"," and what it does is that it takes a look at a machine learning algorithm. It takes a look at the"
"516.72"," data set. Then it tries to create a decision for it and every time it makes a wrong decision,"
"522.24"," a penalty is introduced and whenever it makes a right decision, the penalty is decreased."
"527.92"," Someone has asked, do we also have data reinforcement learning? Just as we have supervised"
"533.1999999999999"," unsupervised learning to train our machine. Yes, we do have data. So many different kinds of"
"539.44"," data can be there. So you could also use reinforcement learning to train a robots."
"545.7600000000001"," So for instance, whenever we're trying to create a robotic interface, we're trying to create a robot"
"550.6400000000001"," that can take decisions that can go someplace and do something. Reinforcement learning takes into"
"557.2"," place. So what it does is that it takes a look at the data that how a person moves, how a person"
"562.48"," interacts and understands how to move according to that. After that, what it does is that it takes a"
"567.2"," look at what are the different situations and how does a person move in those? If the road is"
"572.96"," wet, how does a person move? If the road is not wet, it's quite dry. Then how does a person move?"
"577.9200000000001"," If the road is simply slippery, how the person moves. And using that, if the agent that supervising"
"584.96"," the reinforcement learning algorithm figures out that the robot is deviating from the"
"590.24"," which it's supposed to move, it incurs the penalty. And the model is supposed to decrease the"
"596.0"," penalty so it tries to not repeat the problems that happened before. All right. All right. So now,"
"604.96"," let's move ahead. So now let's take a look at what is classification because that is the kind of"
"610.08"," model that will be building. I'd like to help you understand what classification is. For those of"
"615.76"," you on a wet classification is a machine learning classification is a machine learning technique"
"623.12"," that comes under the supervised learning technique. In supervised learning technique, you can build"
"630.3199999999999"," regression models as well as classification models and will be using taking local classification."
"635.2"," I'll also explain what regression is. So let's take a look at what is classification."
"640.72"," So in machine learning classification is simply an activity that contains the consist of putting"
"647.36"," things into the category based on the similarities of problem criteria. If I give you a lot of people"
"653.2"," who have applied for the loan, who apply, if you're working in a banking industry, you need to"
"658.88"," help understand them what are the kinds of transactions that could lead to fraudulent transactions."
"664.96"," And to do that, you need you can take in some data about the transaction that date the time,"
"671.2800000000001"," the username, the user's credit for phone and so forth. And using that, you can figure out whether"
"676.1600000000001"," or not this transaction is fraudulent or not. So that is the classification problem when you"
"682.08"," already have some categories defined and you want to place the incoming data into one of the"
"688.56"," categories. So there are a fixed set of outputs that your input can map into. For instance, our"
"694.8"," digit recognition is a classic example of classification in which what we do is we take in an image,"
"701.1999999999999"," we analyze it and we take it out whether or not it contains which number does it contain from 0 to 9."
"707.68"," So it could be 0, 1, 2, 3, 4, 5, 6, 7, 8, or 9, any one of those. So there are 10 possible"
"713.76"," classes that are input could go into and that's what we do in classification. So that's what"
"719.68"," classification is. Another problem could be regression. Regression is basically when you're trying"
"724.8"," to predict a continuous value. So think of it this way, when you're trying to predict a continuous"
"729.52"," value, a continuous value would be you're trying to predict the temperature, you're trying to predict"
"739.36"," the housing price based on the features that you have. You're trying to predict the stock market"
"744.88"," price. These are continuous values because it could be a number in fraction, it could be a whole"
"749.1999999999999"," number, it could be a lot of things. There's no particular categories that you can put this into."
"754.9599999999999"," So that's regression and we're learning classification here. So that's the difference between"
"761.04"," two. So let's take a look at common classification problems. The first is character recognition."
"767.92"," So if I give you a, if you've used anything like Google Lens, you probably know what character"
"773.28"," recognition is in which it takes a look at an image and figures out what other things that are"
"778.72"," written in that image. So it could be that it figures out what's written on a board, it could be"
"783.84"," it figure out what's written on a page of paper, so on and so forth. That's character recognition."
"790.48"," Then comes face detection. So if you are unaware of face detection is if you ever used a mobile phone"
"796.24"," with a camera, you put a someone's face in that camera and it creates a bounding box around it"
"802.8000000000001"," to help you help it focus on the person's face and that's what face detection is."
"810.48"," Face detection and face recognition are two different things. Face recognition is understanding"
"814.96"," who that person is. So in face recognition, what we do is good example of it is that"
"822.24"," if the feature that Facebook has deployed in which you upload an image and it is easy and it"
"828.3199999999999"," understands who face is in this image and then gives you the person's name around the bounding"
"837.12"," box and tags them automatically. So that's face recognition. This is face detection."
"843.04"," Then comes fraud detection. So if you work in the backing industry, fraud detection is something that"
"849.04"," is used, fraud detection is something in which machine learning is used quite a lot. It takes a"
"854.4"," look at a lot of transaction, user study scores, so on and so forth and understands what are the"
"859.1999999999999"," different transactions that could be considered fraudulent. Finally, we have intrusion detection."
"864.5600000000001"," This is used in cybersecurity. What they do is they take a look at a lot of logs that it has"
"870.64"," and takes a look at the activity that was performed by particular IP addresses or a range of"
"875.2"," IP addresses in those logs and understands what are the IP addresses that have tried to include"
"880.4"," in our system and if it looks like a person, if it's classified as a person who's trying to"
"885.04"," include in our system, we blacklist their IP addresses which report them to the authorities,"
"889.52"," that's the kind of thing that we do. So someone has asked how do we use machine learning in this"
"893.28"," aspect? How are we using Jupyter in the aforementioned users of machine learning? So Jupyter"
"897.6"," Notebook is a software that will be using but machine learning statements don't really care"
"902.0799999999999"," about how you're creating those. Jupyter Notebook is really useful because it allows us to bundle"
"907.52"," up code with explanations and the output of the code all in one place. So that's how we use that."
"915.12"," But if you were to use plain pison scripts, that would work fine as well. All right, so now we'll"
"920.24"," move on to the next section in which we'll take a look at some popular classification algorithm."
"927.04"," First one is logistic regression. So logistic regression will learn from the data and with that,"
"932.64"," we use a log loss formation and we try and make a prediction about the probability of some particular"
"938.96"," data point being in one of the following categories that we have. So for instance, in our digit"
"945.28"," example, what we do is we have 10 different categories and what logistic regression will do is"
"951.52"," it would take a look at all the data points that we have given it and figure out a probability that"
"957.44"," each data point belongs to one of the following categories. So for instance, if I give it a"
"962.48"," now an image that contains seven, it will pick a probability of let's say around 0.05 that it's 1,"
"969.36"," 0.4, it's 2, 0.3, it's 3 and it goes along so much. And finally, when we go to 7, it has given"
"977.84"," the probability of 0.9, which is the highest, which means that logistic regression algorithms give us"
"983.12"," a bunch of probabilities and the 150 highest probability is the one that we should choose."
"989.28"," That's the basics of it and that's how it makes the prediction."
"994.64"," Okay, so now let's take a look at the other one, which is decision tree. In decision tree,"
"1002.2399999999999"," it's very easy to conceptualize what it does is that it takes a look at the data that we have,"
"1006.8"," figures out columns and values on in those columns and tries to split the data on that. So if I give"
"1013.12"," you, let's say I give you the temperature, I want to figure out whether or not tomorrow it's"
"1021.04"," going to rain or not. So it's going to be classified as either rainy or sunny. In that scenario,"
"1027.2"," we give it the data, it takes a look at the data and figures out what are the different"
"1031.2"," categories, what are the different columns that we have or the features that we have and using"
"1036.56"," those features, how can we split the data so that we can make the decisions better. So for instance,"
"1041.36"," if it takes a look at the, it takes a look at a temperature previous day and then figures out whether,"
"1047.6"," okay, so in the previous days, the temperature was 37 degrees. So on 37 degrees, whether if it's"
"1055.44"," higher than 37 degrees, then it takes a look at other features and if it's lower than 37 degrees,"
"1060.32"," it takes a look at some other features and doing that after looking at all the features and looking"
"1064.8"," coming up with these rules, it figures out, okay, the final answer that I can give you is yes,"
"1070.48"," it's going to rain or yes, it's going to be fine. So then we go to random forest and random forest"
"1076.56"," users multiple decision trees to increase the accuracy of the decision and after getting the"
"1083.84"," result from all the decision trees, the result that has the highest frequency in the decisions that"
"1089.28"," it has gotten from the decision tree is the one that's chosen. So if we get five decision trees and"
"1094.8799999999999"," all out of five of those three of them predict that the number inside an image is seven and"
"1101.12"," others predict the number inside an image is one seven if they output that's given to us."
"1106.3999999999999"," And then comes neve base or naive base, however you pronounce it, it's the kind of algorithm that"
"1111.84"," is used by using a probabilistic model. Inside neve base what we do is we take a look at the"
"1117.84"," probability of something being a mapping to a particular class, it comes up with probabilistic"
"1123.52"," heuristics and then tries to figure out how to use those probabilistic measures to use it."
"1129.36"," Now in case you didn't understand a lot of these algorithms is fine, these algorithms do take a lot"
"1134.24"," of time to digest to understand to implement and as you can see that it could take you a lot of"
"1139.2"," time to understand these algorithms as well as to write these algorithms, it could be very difficult"
"1144.48"," for you. So keeping that in mind, we use scikit learn because it contains all these algorithms"
"1150.0"," prewritten for you so you could just take the data, use these algorithms that have already been"
"1154.96"," built for you and then figure out what's the accuracy and how well they predict your data."
"1159.6000000000001"," So now we finally come to scikit learn. So what is scikit learn? Scikit learn is a free open source"
"1165.68"," machine learning library that contains generic implementation of common machine learning algorithms."
"1171.2"," So as we've already discussed, there are a lot of algorithms that you can use in order to"
"1177.76"," in order to build a machine learning model but writing those algorithms from scratch on yourself"
"1184.0"," by yourself is going to be really really difficult. So using instead of that what we do is we use"
"1191.36"," scikit learn. Scikit learn allows us to understand, allows us to import the algorithms"
"1197.76"," ourselves after understanding the data and we can just use those algorithms that have already been"
"1202.56"," prewritten for us to implement our model instead of us having to implement the algorithms"
"1208.96"," since their common algorithms there have been implemented by people already. So now the thing"
"1214.6399999999999"," that we have to understand is that it contains a lot of functions to perform commonly used operations."
"1218.8799999999999"," So it doesn't just contain the machine learning algorithms that we want to use. It also contains"
"1226.8"," a few functions that we use a lot. So for instance if we have some data set that we want to"
"1232.72"," split it into 70-30 ratio it has a function for that. If you want to figure out the"
"1240.24"," you should want to validate our data set against different distributions of data it has a function"
"1245.3600000000001"," for that. If you want to generate a confusion matrix it has a function for that as well. So there"
"1249.3600000000001"," are a lot of things that it covers. It doesn't cover everything but it has a lot of functionality"
"1254.64"," out of the box that we can use. All right so with that I have another question for you guys"
"1262.24"," hopefully if I show it to you on the polls you'll be able to see it."
"1267.44"," Okay so now I think you can see the question. For those of you who are on aware the question is"
"1272.56"," can you build a machine learning model by only using circuit learn and you know the library."
"1277.12"," So if you are not using any other library other than circuit learn can you build a machine learning"
"1283.04"," model. When I say no other library I mean you are not able to use Python as well to you are not"
"1288.64"," able to write you are not using any other thing other than circuit learn. So for all the tasks"
"1295.5200000000002"," involving machine learning and everything can you use circuit learn only. So the answer"
"1302.72"," that option that we have is yes and no. So I'll wait for the answer and once I've gotten all the"
"1310.96"," answers I will tell you the correct answer. So I encourage everyone who's participating in the"
"1322.24"," all right majority of you have answered and the options that were available were yes and no."
"1329.68"," I'd like to tell you the correct answer which is for the question that can you build a machine"
"1334.72"," learning model with only using circuit learn and no other library. The correct answer is yes you can"
"1341.92"," be people who have answered no I'm assuming that you guys were thinking that because we have to use"
"1346.4"," numpy or pandas or other libraries like that we could probably not be able to use a build a model"
"1353.3600000000001"," without using those libraries. The thing is you can build a model without using those libraries"
"1359.28"," but you would have to do a lot of coding yourself you would have to import the data parts it"
"1364.64"," split it and then use it inside the library of circuit learn but only using circuit learn you can"
"1370.4"," build a machine learning model it's just going to be really difficult and without using circuit learn"
"1374.16"," on any library it's going to be even more difficult. So the correct answer is yes for those of you"
"1381.0400000000002"," who have answered yes you were correct for those of you who answered no you were incorrect but"
"1385.8400000000001"," depending on how your thought process went you were really close. All right so now let's move ahead"
"1394.72"," now close the pool and now let's take a look at why should you use circuit learn."
"1400.64"," First of all it's very easy to use circuit learn is extremely easy to learn and use and it makes"
"1405.76"," the task of implementing mathematical algorithm stability because it contains all the"
"1410.88"," implementation with it we just have to understand how to use the classes and the functions available"
"1415.8400000000001"," in the library. It's very well documented it's got an extensive documentation that contains"
"1422.08"," information about how to use every available feature in the circuit learn library and using that"
"1428.8799999999999"," we can take a look at that. Then it's been created by experts so circuit learn is a library that"
"1436.08"," has been created by expert data scientists who have been inside the data science business for a"
"1440.08"," long time they have written these algorithms themselves and people these people have been writing"
"1447.92"," algorithms that are relating to data science from their hand for a long time so they know how to"
"1454.64"," make a genetic implementation that's many people use. And finally it's very well tested"
"1460.64"," it means use of several developers across the world who contribute patches and bug fixes."
"1465.3600000000001"," circuit learn is an open source library so if you want to take a look at the code underneath"
"1469.2"," the hood you can just go to grid have search for circuit learn or at-colon and you can get the"
"1473.44"," library code yourself. If you find some problem of want to help circuit learn grow and add some"
"1480.88"," features to it you can use grid hub to add some features in the code that the circuit learn has"
"1487.04"," and then suggest a revision to the developers and maintaining because it's open source many people"
"1493.68"," have already submitted a lot of bugs and submitted a lot of bugs so that means it's battle tested as"
"1499.28"," and it has seen a lot of issues already. Now let's take a look at steps in building a classifier"
"1503.84"," this is an important concept all right so now let's take a look at steps in the class if you"
"1508.88"," step in building a classifier so there are five steps in building a classifier but"
"1515.2"," there are several steps in building a classifier a five to be exact but depending on how you're"
"1520.72"," training a model it could be more and could be less usually these are the five generic steps that"
"1525.3600000000001"," are used. The first thing is to import the library so if you're using s-killer and numpy pandas"
"1529.2"," you import it many people import it throughout their code so as soon as they need something"
"1533.92"," only then they'll import it. I like to import everything at the top of my file so that everyone"
"1538.48"," who's using my application knows or looking at the code that I've written knows what are the"
"1543.1200000000001"," libraries that my code depends on. So you can firstly import all the libraries needed to train"
"1549.44"," and build the model that you need and these libraries can perform tasks training from numerical"
"1554.16"," manipulation such as numpy and test in the accuracy that we'll be using second-line code."
"1561.2"," Then what we do is we import the data set so data set is the data that we will be training"
"1566.64"," our model on it's important to understand what data set will be used in how we'll be using it"
"1571.68"," so that's how we firstly need to import the data set or we can after important data set we view"
"1577.52"," the data set we load it we manipulate it so that it's clean and it's in a state that we can"
"1582.08"," use it to train our model and do this following steps and after that we can use it in machine"
"1586.6399999999999"," learning models. After that what we do is split the data so when we get the data set it's"
"1591.6"," action-wide variables are right for each other so the thing that we are using to make the prediction"
"1596.08"," and the prediction itself are right inside the data first it splits that and then we split our data"
"1602.56"," into a training and testing set. Training set is used to train the model testing set is used to"
"1607.36"," test the model and using that we understand what is the how the model has learned and what is"
"1612.6399999999999"," the accuracy that it's giving us. Then comes training the model in which is the task of the"
"1617.84"," SI second-learn library it creates algorithm and allows us to feed it data so that it can learn from"
"1622.7199999999998"," the data and after learning from the data it can do the classification or regression tasks"
"1629.1999999999998"," that are needed. Finally we test the model we give it the data that we had set aside for testing"
"1635.36"," after giving it the data we ask it to make the classification and depending on how many"
"1639.12"," was correctly classified we give it a percentage from 0 to 100. Higher percentage means our model is"
"1645.6"," more accurate lower percentages means less accurate. It is a max percentage of a model"
"1650.24"," accuracy the max percentage is 100 percent but no models can be expected to have 100 percent accuracy."
"1655.52"," If a model when training or testing it gives 100 percent accuracy it could be that your training"
"1660.48"," set is too small that is it either contains only 10, 20, 40 or 100 values or it could be that your"
"1667.2"," training set is overfitted which is the variance issue that we have talked about recently."
"1673.92"," You need to train the model again and if it sees some data that it hasn't already seen it will"
"1677.92"," give four results. Someone has asked is someone did you learn splitting the data sets in wine house?"
"1683.3600000000001"," So splitting the data set is basically the task when we get the data and we want to set aside some"
"1688.4"," data that our model can be trained on and the remaining data is used to test whether or not our"
"1694.3200000000002"," model is giving us correct accuracy. So in supervised learning we already have the data and the"
"1699.1200000000001"," output that should map to once we see it in the data and we take a look at the predicted output"
"1704.48"," and the correct output the percentage of outputs that match are the ones that are correctly classified"
"1711.04"," and the ones that are correctly classified are the outputs that lead to higher percentage of"
"1717.6000000000001"," accuracy. So someone has asked to explain some example on training the model."
"1721.3600000000001"," Training the model basically means just taking the algorithm from the cycle learn"
"1725.76"," cycle learn library. Feeding in the data which is the x and y variables that we have created for"
"1730.72"," training then that data that algorithm learns from the data and after that we can make predictions"
"1738.56"," we'll take a look at that in the demo. All right hopefully you can see the question the question"
"1745.04"," yeah yeah so someone the question is which set is used to validate the performance of the trained"
"1751.28"," model. So once the model has been trained which set is used to check whether or not our models"
"1756.0"," accuracy is good. So I'll wait for a few moments and you guys can answer in the poll and submit"
"1761.6799999999998"," your answers. Whether it's training set or testing set training set is the first option testing set is"
"1767.6799999999998"," the second option. All right in the meantime people have asked a few questions I'd like to answer"
"1772.32"," those. Someone has asked what is the ratio to split the training and testing set. So for training"
"1777.04"," and testing you can split the data into depending on how large your data set is if it's really large"
"1782.0"," like you have millions of billions of rows then you can just take 1% or 10% of your data. If it's a"
"1787.84"," small data set then 70-30 is a good ratio to split your data. It's testing set which is the"
"1793.04"," correct answer. The training set is used to train the models the testing set is used to figure out"
"1796.8799999999999"," whether or not the answer is whether or not the model is performing well which is why we use it"
"1802.56"," and let's have our hands on we'll have a demo here. So I already have prepared a Jupiter node to"
"1810.48"," four you guys in which we perform all the five steps. So let me just first explain what we're doing."
"1816.16"," We're using the iris data set that contains information about flowers. It contains information"
"1821.28"," about the shaper length of a flower, shaper width of the flower, petal length of the flower and petal"
"1827.12"," length and petal width of the flower. Using these four features we're trying to predict the kind of"
"1832.16"," flower that it contains or kind of the iris flower that it's containing. There are four kinds of"
"1837.36"," flowers that our data set has. Sorry three kinds of flowers that it has. Virginia, Catoza and these are"
"1844.56"," different kinds. So what you do is firstly we import all the libraries. As you can see the"
"1850.56"," libraries were easily imported so it works fine. From Eskiland I'm importing the load iris function"
"1857.1999999999998"," that will get us the data set. It's a very common data set and so that you can follow along with me"
"1862.1599999999999"," and you don't have to wait to load the data from sub-report data source. You can just use"
"1867.76"," this code that I'm showing you on my screen right now. Then we are using the"
"1874.7199999999998"," train test displayed function which will use the splitter data set into the specified ratio."
"1880.0"," Someone has asked can we try it on Google colab? Yes you can try it on Google colab. That's a"
"1884.0"," very good use case for Google colab actually. If you want to use Google colabs or if you want to"
"1889.84"," use your local local food foods you can use those as well. With that we have a train test"
"1895.6"," display which is a function that is used to split the data. We will be using the Logistic"
"1899.6"," Regression model which we can import from Eskiland.Linium model. Finally we will be taking a look at"
"1906.3999999999999"," the accuracy score as well and Psychic Run already gives us a function that does it for us."
"1912.32"," I'll be more than happy to answer you on the questions that you ask about the"
"1918.9599999999998"," about the code that you're watching. If you want to follow along you can type in the code in your"
"1925.6"," Jupyter Notebooks. If you have it, if you don't have Jupyter Notebooks then it's going to be"
"1929.84"," difficult for you. You can use Google colab. That's a really good option for you to try if you want to"
"1936.6399999999999"," follow along. So now let's take a look at it. So someone has asked that they need to import the"
"1942.8799999999999"," data set should they need to import it with its name. So we're using the load IRIS function"
"1949.1999999999998"," and using that you can just import it. You don't need to specify any name. If you have a"
"1954.0800000000002"," CSV file on your local system you're importing that you would need pandas library and need to"
"1959.1200000000001"," know the file and the example. Okay. So now we load the data set."
"1969.6000000000001"," But now let's take a look at what the shape of the data set is. As you can see it's a dictionary."
"1974.64"," dictionary is a key value pair. It's a common Python data structure. It contains a key of data that"
"1982.48"," contains all the data. And it's 5.1 3.5 1.4 0.2. These are individual rows in our data set."
"1990.64"," Now we don't know what these values mean. So taking a look at the data we have targets as well."
"1996.32"," Target ranging from 0 1 and 2. Now let's take a look at the target name. These are the different"
"2002.8"," types of class that will be predicting. These are the categories that will be predicting."
"2006.96"," Satosa, Vasi color and Vajinika. So 0 means Satosa. One means Vasi color. Two means Vajinika."
"2013.2"," The reason why we have encoded them into numbers is because machine learning algorithms only"
"2017.76"," understand numbers. They don't understand words or strings or letters or sentences. So we need to"
"2023.6000000000001"," first convert those numbers into classes into numbers and then we can make the prediction."
"2029.68"," And now for our columns, we want to be able to understand what are the different columns that we"
"2036.96"," have. And what we get here is we get the separate separate width by the length and"
"2043.44"," battle width. So it's given us the example of what we have. So feature names are the first column"
"2049.2799999999997"," has separate length. Second column has separate width. Third column has battle length. Fourth column"
"2055.04"," has battle width and all of them are in centimeters. So that helps us understand the data."
"2059.92"," Now it's time to split the data. Which is written in x and y. We'll be using these features."
"2071.04"," And using those we'll be predicting these targets. 0 1 or 2 which map to Satosa, Vasi color,"
"2077.2"," or Vajinika. So with that, after splitting the data, I try to split it into 70-30 ratio."
"2085.4399999999996"," So let me just explain what that means. Using the data set that we have, the data.data.data."
"2096.16"," The data key inside it contains all the data that we need. So let me just show you what the"
"2101.76"," size of the data is. Penetrate a row below and look at the length of x. We have 150 rows there."
"2111.1200000000003"," Now if I split it into a 70-30 ratio. Right. And now if I insert a row and I print"
"2121.84"," the length of x train, we have 105. So 70% of 150 is 105. And remaining 45 are for testing."
"2139.36"," So we will use 70% of our data for training, 30% for testing. And with that, we create a"
"2145.68"," logistic regression model. This is a class in S-Klun. It just takes in the logistic regression"
"2152.56"," model and creates an object of it. We use the fit method to fit our data. Fit basically means"
"2157.8399999999997"," train our data. We pass in the training x-value and training y-variable. So the 105 rows that we"
"2165.04"," have created for testing, we pass it inside our training and testing set. If you're following along,"
"2171.44"," if you want, we just scroll up to help you look at the code. This is what the first line,"
"2176.2400000000002"," first things we import is. Then we load the dataset. You only need to write this code. This is just"
"2183.6"," for explanation. This is mark down format. So you don't need to copy that. After we have trained"
"2188.8"," our data, since our dataset is small, it trains in a few milliseconds. Now we make the prediction."
"2195.44"," And after the predictions are made, we take a look at the accuracy. An accuracy is 88%. We just"
"2200.16"," get quite good. Because machine learning models use statistics, these accuracy could change."
"2207.44"," So if I train it again and then I check it again, it's giving me 100% accuracy."
"2214.0"," Because our dataset is too small. And our scheme is 91% accuracy. So it's giving us high accuracy,"
"2221.92"," but the accuracy is variable. The reason why is because depending on how data split,"
"2226.16"," when we split the data using trained test, it splits it randomly. So it takes a look at 45"
"2231.7599999999998"," values randomly and split it into another 45 and 105 random values from the entire dataset."
"2239.04"," And then we can train the model. So someone has asked us, space recognition, classification or"
"2244.3199999999997"," space recognition is a supervised learning problem. It's a classification problem, but it takes"
"2251.7599999999998"," a look at a very different technologies that are not just part of classification. It takes a"
"2256.56"," bit of computer vision, handling the data. There are many other different kinds of things."
"2262.24"," Space detection is a classification algorithm. Space recognition is a system that uses"
"2269.7599999999998"," classification algorithms along with other algorithms as well. This computer vision in"
"2274.64"," area of ML years it is. Thank you so much for joining me and hopefully this has been informative."
