"start","content"
"0.0"," Hello guys and welcome all to the spythensurfacation course."
"8.28"," In our previous session we learnt some of the theoretical concept of machine learning."
"12.16"," We covered the topics like what is machine learning, what are its various types and what"
"15.96"," are its various use cases."
"17.56"," I hope those things are clear to you."
"19.2"," So now we'll move ahead and perform a hands-on on machine learning."
"22.240000000000002"," We'll be loading a data set, understand its structure using statistical summary and finally"
"27.2"," create six different machine learning model and pick the best of it."
"30.56"," Okay, well there's a little piece of advice for you guys."
"33.4"," If you really want to learn machine learning with Python, so I'd suggest you to complete"
"37.08"," an end-to-end project."
"38.4"," At least it will force you to install and start your Python interpreter."
"41.92"," It will even give you a bird's eye view of how to step through a small project and"
"46.4"," once you complete one of the project you'll get a confidence and maybe you can go on and"
"51.120000000000005"," start with your own small projects."
"53.6"," So as a big enough, when you're applying machine learning to your own data set, you are"
"57.2"," working on a project."
"58.44"," So a machine learning project may not be a linear."
"60.760000000000005"," It may have a number of steps like define the problem, prepare the data, evaluate the"
"65.18"," algorithm, improve the result and present the results."
"67.84"," So the best way to really come up to terms with a new platform or tool is to work through"
"72.6"," a machine learning project end-to-end and cover the key step."
"75.92"," Namely, loading the data, summarizing it, evaluating algorithm and making some predictions."
"80.48"," So once you complete it, you'll have a template of your own that you can use on data set"
"84.36"," after data set."
"85.36"," So you can fill in the gaps such as further data preparation and improving results task"
"89.32000000000001"," later once you have more confidence."
"91.12"," Okay, so let's move ahead."
"92.92"," Things you'll learn after this session."
"94.36"," So once we are done with this session, you'll know how to load the data set, how you can"
"97.84"," summarize it, how to visualize your data set, how to evaluate some algorithm and finally"
"102.48"," you'll learn how to make some predictions."
"104.64"," Okay, so let's move ahead."
"106.32000000000001"," So we'll be performing the demo on irisflower data set."
"109.16"," Well, the project which we are going to perform is generally known as the hello world of"
"113.32"," machine learning."
"114.32"," So it's the best project to start with because it's so well understood."
"117.52"," Okay, and why it's so well understood because so all the attributes in this data set is"
"122.2"," numeric."
"123.2"," So all you have to do is figure out how to load and handle the data."
"126.03999999999999"," So it's a classification problem."
"127.84"," Thereby allowing you to practice with perhaps an easy type of supervised learning."
"131.88"," Okay, it also supports a multi-class classification problem that may require some specialized"
"136.84"," handling."
"137.84"," So it's iris data set consists of four different attributes, sepil and sepil with petal"
"141.84"," and petal with and it consists of 150 rows, meaning it is very small and it can easily fit"
"147.72"," into the memory."
"148.72"," All the numeric attributes are in the same unit and the same scale."
"152.2"," Okay, so this data set doesn't require any special scaling or transformation to get"
"156.68"," started."
"157.68"," Okay, so let's get started to your hello world machine learning project in python."
"161.12"," So this would be a step by step tutorial."
"163.6"," So let's get started with your hello world machine learning project in python."
"166.8"," Okay."
"167.8"," So the very first thing that I'll be doing up here is importing all the required libraries."
"172.88000000000002"," So I'm importing pandas and from pandas I'm importing scatter matrix."
"177.24"," Next I'll be importing map dot live as PLT."
"180.4"," Then we are importing model selection from escalon classification report from escalon."
"185.56"," Metrics confusion matrix from escalon."
"188.28"," Matrix accuracy score again from escalon."
"191.68"," These are various machine learning models."
"196.12"," Okay."
"197.12"," And classifier linear discrimant analysis, Gaussian,"
"200.6"," base and support vector machine."
"202.88"," Okay."
"203.88"," So yeah, importing all these model from the on-psych it learn library."
"207.08"," Okay."
"208.08"," So let's execute it."
"209.24"," And next step is to load the data set."
"211.44"," So the very first parameter that I'm passing up here is the URL of my data set."
"215.48000000000002"," Okay."
"216.48000000000002"," Or the path of my data set basically."
"218.20000000000002"," So this is my data set just to show you."
"221.92000000000002"," So this link consists of my data set."
"224.16"," Okay."
"225.16"," Next I'm defining an array as names."
"227.08"," This consists of the name of the attribute."
"229.4"," Zeppel length, Zeppel width, Petal length, Petal width and class."
"232.92000000000002"," Okay."
"233.92000000000002"," And next I'll be defining a variable as data set."
"236.48000000000002"," And next I'm defining a variable data set."
"238.56"," So data set equal pandas dot read CSV URL, common names, equal names."
"243.24"," So what it is basically doing it is fetching the data from here from this link URL from"
"248.60000000000002"," this URL."
"249.60000000000002"," And it is giving the name of the attribute as all these Zeppel length, Zeppel width,"
"253.84"," Petal length, Petal width and class."
"255.88000000000002"," Okay."
"256.88"," So basically load your data set."
"258.32"," Fine."
"259.32"," Let's execute it."
"260.32"," So our data set is loaded."
"261.32"," Now let's summarize the data set."
"262.84"," Now let's check how many rows and columns do our data set have."
"265.71999999999997"," So let's execute this and check the shape of our data set."
"268.52"," So our data set consists of five different columns and 150 rows."
"272.28"," Okay."
"273.28"," Next if you want to check the sample data set then you have a head function over here."
"276.4"," So this will give you first 20 result of your data set."
"279.2"," Okay."
"280.2"," Zero to 90."
"281.2"," Fine."
"282.2"," Next is the data set dot describe function."
"284.44"," So this will give you the various description from your data set like count, mean, standard"
"289.48"," deviation, minimum, percent, 25 percentile, 50 percentile, 75 percentile and max."
"295.36"," So count is basically the total count of Zeppel length, Zeppel width, Petal length and"
"298.96"," Petal width."
"299.96"," Okay."
"300.96"," Similarly mean of all similarly standard deviation and so on."
"303.36"," Okay."
"304.36"," Next, what if I want to check how many different classes are there or how many values"
"308.04"," contains in each class."
"309.56"," So we'll group by the class and we'll find the size of each class."
"313.88"," Okay."
"314.88"," And it's executed."
"315.88"," So we got the output up here as either set osa, 50, iris, vertical or 50 iris, virginica,"
"322.0"," also 50."
"323.0"," So this means that we have three different classes at a set osa, vertical or an virginica."
"327.96"," All three of them consist of 50 values."
"330.72"," Okay."
"331.72"," So next is data visualization."
"333.92"," So now that we have some basic idea about our data, let's create some visualization out"
"337.88"," of it."
"338.88"," So we're going to look at two different types of plot."
"340.92"," The first would be a univariate plot that is to understand about each attribute and"
"345.4"," next would be the multivariate plot to understand the relationship between the attributes."
"350.08"," Okay."
"351.08"," So first let's plot a univariate plot that is plot of each individual variable."
"355.6"," Okay."
"356.6"," So this is the univariate box and viscous plot."
"358.32"," So let's consider just Zeppel length."
"360.2"," So the bottom part of here, this represents the minimum value and this is the maximum value."
"364.96"," Okay."
"366.32"," And this green bar over here represents the mean value."
"369.28"," So if you see the mean of Zeppel length, it's around 5.84."
"373.03999999999996"," It's all representing the same."
"374.44"," So it's 5.84."
"375.76"," Okay."
"376.76"," These circles ever when below the minimum point are nothing but the outliers."
"380.79999999999995"," Okay."
"381.79999999999995"," Yeah."
"382.79999999999995"," One more thing."
"383.79999999999995"," The minimum and the maximum value, right?"
"384.79999999999995"," So minimum is 4.3 for Zeppel length and maximum is 7.9."
"387.79999999999995"," Okay."
"388.79999999999995"," So the minimum value you got up here is 4.3."
"391.2"," Okay."
"392.2"," So this is my 4.3 minimum value."
"393.88"," And here we have the maximum value as 7.9."
"396.48"," Okay."
"397.48"," And these are the outliers, these circles."
"399.6"," Similarly for Zeppel width, my minimum value was Zeppel width was 2.0 and maximum 4.4."
"405.36"," So minimum from here is 2.0, this value, it's outlier."
"409.48"," And the maximum value is 4.4, this value, 4.4."
"413.84"," It's also outlier."
"414.84"," And the mean value of Zeppel width is 3.05."
"418.0"," So we got mean somewhere around here."
"420.32"," Okay."
"421.32"," In order to understand our data in a better way, we created a univariate box and viscous"
"425.8"," plot."
"426.8"," Okay."
"427.8"," This gives us a much clear idea about the distribution of the input attributes."
"430.88"," Okay."
"431.88"," Well, if you want, you can even create a histogram of each input variable to get an idea of the"
"436.0"," distribution."
"437.0"," Okay."
"438.0"," So let's execute it."
"439.0"," So there's my histogram."
"440.0"," So in Zeppel length and Zeppel width, I can find the Gaussian distribution up here."
"443.8"," Okay."
"444.8"," So there's a useful to note as we can use algorithms that can exploit this assumption."
"448.56"," Okay."
"449.56"," So this is the multivariate plot."
"451.2"," Well, multivariate plot is used to check the interaction between the variables."
"455.44"," Okay."
"456.44"," So let's just execute it and see what's the output."
"459.72"," So here's a scatter plot of all pairs of attribute."
"462.84000000000003"," So this can be helpful to spot structured relationship between input variables."
"467.08"," Okay."
"468.08"," So as you can see, we have diagonally grouping of some pair of attributes."
"470.4"," So this suggests a high correlation and a predictable relationship."
"473.84000000000003"," Okay."
"474.84000000000003"," So from here, you can say that Zeppel length is more dependent on Zeppel width as compared"
"479.52"," to petal length and petal width."
"481.71999999999997"," Similarly, same thing over here, right?"
"483.91999999999996"," So petal length is more dependent on Zeppel width as compared to Zeppel length and petal"
"489.76"," width."
"490.76"," Okay."
"491.76"," And the petal width, it's more dependent on Zeppel length as compared to Zeppel width"
"495.44"," and petal length."
"496.44"," Okay."
"497.44"," So these are the things that we can conclude after visualizing our data."
"500.68"," Okay."
"501.68"," Now it's time to create some model and estimate the accuracy on unseen data."
"505.24"," Okay."
"506.24"," So here we'll follow step by step procedure."
"508.47999999999996"," Okay."
"509.52000000000004"," So we'll start out a validation dataset that is divide our data set into train and test"
"513.28"," part."
"514.28"," Then we'll use a tenfold cross validation technique that will randomly distribute our data"
"518.76"," into ten different data sets."
"520.4"," Okay."
"521.4"," So that we can perform training and testing on each part."
"523.6"," Then we can finally combine the result of all of them to get a better accuracy."
"527.84"," Okay."
"528.84"," Then we are going to build five to six different models to predict species from char measurement."
"532.6800000000001"," Okay."
"533.6800000000001"," And then finally, we'll select our best model."
"535.28"," So our first step is creating a validation dataset."
"537.84"," That is dividing our data set into train and test."
"540.8000000000001"," So here what we are doing, dividing our data set and train and test."
"548.2"," Array equal data set dot values."
"550.2"," So it consists of all the values inside your data set."
"552.9200000000001"," Here you're defining x equal array starting."
"556.4"," So here you're taking a portion of the array."
"558.76"," Okay."
"559.76"," And you're placing it in x."
"560.9200000000001"," So what is that?"
"561.9200000000001"," So your row up here is nothing."
"564.12"," So it will start from the very first row and the column 0 to 4."
"568.92"," So it will take up to fourth column."
"571.12"," So four column from the first row."
"573.52"," That is x is storing all the name of the attribute."
"576.24"," Fine."
"577.24"," Similarly, we are defining a variable y."
"578.68"," So y will consist of the fourth column."
"580.8"," Next, I'm defining a variable y array of starting from first row and fifth column."
"586.08"," Okay."
"587.08"," As 0, 1, 2, 3, 4."
"588.64"," So here number four represents my fifth column."
"591.6800000000001"," Okay."
"592.68"," So here, I'm defining a validation size as 0.20."
"595.64"," So here, validation size means it will split my data as 80% in one part and 20% in other."
"602.4799999999999"," Okay."
"603.4799999999999"," So next we are defining a seed value."
"605.12"," So the seed value is used to initialize a randomization saving or setting it to same"
"610.3199999999999"," number each time."
"611.3199999999999"," Caron T is that every time you execute the algorithm, it will come up with the same result."
"615.88"," Okay."
"616.88"," Next we are defining scoring as accuracy and here we are defining x train x validation"
"622.28"," and y train and y validation."
"624.1999999999999"," So here we are defining x train x validation, y train and y validation."
"628.64"," So x train would be my training data from this part and x validation is the testing data"
"635.04"," from same."
"636.04"," Okay."
"637.04"," So according to my above code, my 80% of the data will lie in the training part and"
"641.68"," the rest 20% would be treated as a validation data or the test data."
"646.12"," As I have already mentioned, the validation size is 0.20 or 20%."
"650.12"," So this means that 20% of my data is test data."
"652.68"," Similarly, we will define y train and y validation."
"655.72"," That is 80% of the training data from this array and rest 20% of the testing data from"
"661.5600000000001"," same array."
"662.5600000000001"," Okay."
"663.5600000000001"," So here we are basically splitting our data set."
"665.32"," Okay."
"666.32"," On the base of x, y test size equal validation size that we have already specified as 0.20"
"671.76"," and my random state equals c and my random state is c."
"675.52"," So every time I executed, the algorithm will come up with the same result."
"679.68"," So now that we have created a split among our data set and we have divided our data set"
"684.4799999999999"," into train and test."
"685.76"," So our next step would be to build our model."
"688.16"," Okay."
"689.16"," So this is how we are building our model."
"691.0"," Okay."
"692.0"," So from the circuit loan library, we are calling logistic regression."
"695.1999999999999"," We are calling linear discriminant analysis, KNN, division tree, Gaussian, a base and"
"700.52"," SVM or support vector machine."
"702.3599999999999"," Okay."
"703.3599999999999"," Here what we are doing, we are creating 10 folds."
"705.16"," Okay."
"706.16"," So 10 folds."
"707.16"," Let me just go step by step."
"708.48"," So here we are defining a variable as K fold."
"710.5600000000001"," So K fold equal modern as core selection."
"713.04"," K fold how many split we want to perform is 10 split and random state again."
"717.96"," See that is seven."
"718.96"," So every time it will perform the same random split."
"721.8000000000001"," Okay."
"722.8000000000001"," So this is how my data would be split into 10 different parts."
"725.4"," Okay."
"726.4"," Now once the data is split into 10 parts, then we'll define a variable as CV underscore"
"731.04"," results."
"732.04"," Okay."
"733.04"," So CV underscore results equal model selection dot cross validation score inside that we"
"737.08"," are passing a model, what model we are choosing."
"739.6800000000001"," We are passing the X train data, the Y train data, CV equal K fold, whatever the value of"
"745.0"," K fold is there and then finally scoring, which we have initialized up here as accuracy."
"750.4000000000001"," Okay."
"751.4000000000001"," Then what we are doing, we are appending our result."
"753.72"," So a CV result would be appended to results that is to this list."
"757.84"," So next we have is result dot append CV underscore result."
"761.24"," So this list would be updated with the CV underscore result one."
"765.24"," And here we are pending the name and finally we are printing a desired result."
"769.44"," So one by one, this will go in a loop."
"771.48"," Okay."
"772.48"," So every time this loop execute, you will get the accuracy of one of the model."
"775.96"," Okay."
"776.96"," So here right now our model consists of all these values."
"780.8"," Okay."
"781.8"," As we are appending it, that is we are adding all these values one by one to our what"
"785.64"," at consists of logistic regression, legendary, Gaussian, Nebeson support vector machine."
"790.6800000000001"," All of them are in our model."
"792.2"," Okay."
"793.2"," And finally it is pending the message."
"795.16"," Let's check the output."
"796.7199999999999"," So we got the output as this."
"798.8"," So here the accuracy of a logistic regression is 0.96 that of linear discriminant analysis"
"804.76"," is 0.97 can and 0.98, 10 entry 0.96, Nebes 0.97, support vector machine as 0.99."
"813.36"," So from this we can say that support vector machine is having the highest accuracy."
"817.76"," Okay."
"818.76"," Now our next step would be to compare the algorithm and select the best model."
"821.84"," So just by looking at this output, we can say that support vector machine is having the"
"825.4"," highest accuracy."
"826.6"," But let's compare all these algorithm and see which one fit the best."
"830.6"," So for that I'll be plotting a box and viscous plot for the accuracy versus the name of the"
"835.32"," algorithm."
"836.32"," Okay."
"837.32"," So there's my algorithm comparison."
"838.32"," Here I have the name of my algorithm and this is the accuracy for the green point which"
"843.2800000000001"," you can see up here is nothing but the mean values or this value."
"847.9200000000001"," Okay."
"848.92"," So in case if you are removing these outliers like this one, these two and this."
"858.92"," So by removing them, KNN will give you the 100% accuracy."
"863.92"," Okay."
"864.92"," So if you are removing these outliers, this one, these two and this."
"876.36"," So by removing them, KNN will give you the 100% accuracy result."
"880.16"," By removing these two outliers, the best would give you the highest accuracy result or"
"884.6"," 100% accuracy result."
"886.52"," So even in case of SVM, if you remove this outlier, you'll get a model with 100% accuracy."
"891.8000000000001"," Okay."
"892.8000000000001"," Now next is making the prediction."
"894.44"," Well, just accuracy shouldn't be your metric to decide that this is your best model."
"899.44"," Okay."
"900.44"," So for selecting a model, you have several other features which you should decide apart from"
"904.64"," accuracy."
"905.72"," So your accuracy will not always be the metric to select the best model for."
"909.76"," Okay."
"910.76"," So let's move ahead and see how to make the prediction."
"912.88"," So we have already calculated the accuracy, right."
"915.56"," So now from this graph, we know that support vector machine is tearing the highest accurate"
"920.3199999999999"," result even with the outlier."
"922.08"," If this outlier is removed, then it's the best one."
"924.28"," Okay."
"925.28"," Now let's make some prediction on it."
"926.28"," Okay."
"927.28"," So let's execute this."
"928.28"," So here we are printing the accuracy score."
"930.2"," We are printing the confusion matrix and the classification report for Y validation and"
"935.0"," prediction that is testing and predicted value."
"937.88"," So here we are printing the accuracy score for the tested value and the predicted value."
"942.6"," We are printing our confusion matrix for the tested value and the predicted value and"
"946.16"," finally a classification report, same for the tested value and predicted value."
"950.36"," So if you check the score for tested value and predicted value, like accuracy score while"
"954.96"," using SVM or support vector machine is 0193."
"958.24"," So there's my confusion matrix up here."
"960.36"," And finally we have a precision, recall, FN measure and support data for all these three"
"966.0"," classes."
"967.0"," RSETO, IS, VERSICALO and ISVORGENICAL."
"969.6"," So let me just explain you what is precision, what is recall, what is FN score and support."
"973.76"," So this precision, it tells how accurate or precise is a model."
"978.28"," That is out of those predicted positive, how many of them are actual positive."
"982.92"," So this is precision."
"984.24"," Okay."
"985.24"," Next is recall."
"986.24"," It calculates how many of the actual positives our model has captured through labeling it"
"990.84"," as positive."
"991.84"," Okay."
"992.84"," Next is FN score."
"993.84"," It is used when you want to seek a balance between precision and recall because of a large"
"997.44"," number of actual negative values."
"999.6"," Okay."
"1000.6"," And finally we have support, which is the number of sample of the true response that lies"
"1004.44"," in that particular class."
"1005.84"," Okay."
"1006.84"," So while predicting with SVM, we had seven true responses in case of ISSETO, so 12 in case"
"1012.48"," of ISVORGENICALO and 11 in case of ISVORGENICAL."
"1016.8000000000001"," Okay."
"1017.8000000000001"," Similarly, I made prediction using SVM model."
"1019.5600000000001"," If you make prediction using K&N, you will get a different result of here."
"1022.84"," In this case, we got the accuracy as 0.9 and the precision score, recall score, FN score"
"1028.48"," and support score, all are different."
"1030.32"," Okay."
"1031.32"," So here in case of K&N, you can see that we have the accuracy as 0.9 or 90%."
"1036.88"," The confusion matrix provides an indication of the three errors that we made."
"1040.52"," Finally, the classification report provides a breakdown of the class by precision, recall,"
"1044.84"," FN score and support."
"1046.44"," Okay."
"1047.44"," So we use Python to create few machine learning model and select the best model out of them."
"1051.68"," So our best fit model is SVM as it is giving the highest accuracy and high values of precision"
"1058.0"," and recall."
